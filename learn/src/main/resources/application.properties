spring.application.name=learn
server.port=8081
# LLM integration
# llm.api.url: set to your LLM endpoint. Default is set to a Gemini/Vertex-style generate endpoint.
llm.api.url=https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-pro:generateContent
# llm.model: model to request (default: Gemini 2.5 Pro)
llm.model=gemini-2.5-pro
# request timeout in seconds
llm.request.timeout-seconds=300

# Cache config (Caffeine) - optional tuning
spring.cache.type=caffeine
spring.cache.caffeine.spec=maximumSize=100,expireAfterWrite=6h

# Provider hint: set llm.provider=gemini|openai
llm.provider=gemini
